basic_config: &basic_config
  # Run settings
  log_to_screen: !!bool True # Log progress to screen.
  save_checkpoint: !!bool True # Save checkpoints
  checkpoint_save_interval: 10 # Save every # epochs - also saves "best" according to val loss
  true_time: !!bool False # Debugging setting - sets num workers to zero and activates syncs
  num_data_workers: 2 #6 # Generally pulling 8 cpu per process, so using 6 for DL - not sure if best ratio
  enable_amp: !!bool False # Use automatic mixed precision - blows up with low variance fields right now
  compile: !!bool False # Compile model - Does not currently work
  gradient_checkpointing: !!bool False # Whether to use gradient checkpointing - Slow, but lower memory
  exp_dir: './Dev_BLASNET' # Output path
  #exp_dir: './Dev_BLASNET_SViT' # Output path
  #exp_dir: './Dev_BLASNET_AViT' # Output path
  log_interval: 1 # How often to log - Don't think this is actually implemented
  pretrained: !!bool False # Whether to load a pretrained model
  # Training settings
  drop_path: 0.1
  batch_size: 64  
  max_epochs: 1500
  scheduler_epochs: -1
  epoch_size: 44 
  rescale_gradients: !!bool False # Activate hook that scales block gradients to norm 1
  optimizer: 'AdamW' #'adan' # adam, adan, whatever else i end up adding - adan did better on HP sweep
  scheduler: 'steplr' # Only cosine implemented
  warmup_steps: 100 # Warmup when not using DAdapt
  learning_rate: 1e-4 # -1 means use DAdapt
  weight_decay: 1e-3
  n_states: 15 #12   # Number of state variables across the datasets - Can be larger than real number and things will just go unused
  state_names: ['Pressure', 'Vx', 'Vy', 'Density',  'Vx', 'Vy', 'Density', 'Pressure'] # Should be sorted
  dt: 1 # Striding of data - Not currently implemented > 1
  difference_learning: !!bool False # Whether to use difference learning
  curriculum: 0 # Number of epochs to use for curriculum learning - 0 means no curriculum. If data loader is random, this makes no sense.
  curriculum_steps: 10 # Number of steps to use for curriculum learning
  data_augmentation: !!bool False # Whether to use data augmentation - 3D flip and rotation
  cubic_interp: !!bool True # Whether to use cubic interpolation for upsampling, if false uses linear
  grad_loss_alpha: 0.99 # Weight for gradient loss term
  leadtime_max: 0 #prediction lead time range [1, leadtime_max]
  n_steps: 1 #16 # Length of history to include in input
  enforce_max_steps: !!bool False # If false and n_steps > dataset steps, use dataset steps. Otherwise, raise Exception.
  accum_grad: 1  
  # Model settings
  model_type:  'turbt' # no need for time_type and space_type inputs
  # model_type:  'vit_all2all' # no need for time_type and space_type inputs
  #model_type:  'svit' #currently only support time_type=="all2all_time" and space_type=="all2all"
  #time_type: 'all2all_time' # 
  #space_type: 'all2all' #
  #model_type:  'avit' #currently only support space_type=="axial_attention" and time_type=="attention"
  #time_type: 'attention' # 
  #space_type: 'axial_attention' #
  tie_fields: !!bool False # Whether to use 1 embedding per field per data
  embed_dim: 192 # Dimension of internal representation - 192/384/768/1024 for Ti/S/B/L
  num_heads: 3 # Number of heads for attention - 3/6/12/16 for Ti/S/B/L
  processor_blocks: 12 # Number of transformer blocks in the backbone - 12/12/12/24 for Ti/S/B/L
  tokenizer_heads:
    - head_name: "tk-3D"
      patch_size: [[2, 2, 2]]
  SR_ratio: [8, 8, 8]
  hierarchical:
    filtersize: 2
    nlevels: 4 #[2^3, 2^2, 2^1, 2^0]
  notransposed: !!bool False # when True, do not use conv_transposed due to checkerboard artifacts; Default, False
  sp_groupsize: 8 #sequence_parallel_groupsize
  sts_model: !!bool False
  sts_train: !!bool False  #when True, we use loss function with two parts: l_coarse/base + l_total, so that the coarse ViT approximates true solutions directly
  #gammaref: 0.2 #pick all tokens that with variances larger than gammaref*max_variance to refine
  #refine_ratio: 0.2 #ratio of coarse tokens picked to be refined
  bias_type: 'PositionAreaBias'  # Options rel, continuous, none, PositionAreaBias
  bias_MLP: !!bool True
  # Data settings
  augmentation: !!bool False # Augmentation not implemented
  use_all_fields: !!bool True # Prepopulate the field metadata dictionary from dictionary in datasets
  tie_batches: !!bool False # Force everything in batch to come from one dset
  extended_names: !!bool False # Whether to use extended names - not currently implemented
  embedding_offset: 0  # Use when adding extra finetuning fields
  train_data_paths: [
              ['/pscratch/sd/c/csalah/data/BLASTNET/train_data_summary.csv', 'SR', '', "tk-3D"],
              ]
  valid_data_paths: [
              ['/pscratch/sd/c/csalah/data/BLASTNET/val_data_summary.csv', 'SR', '', "tk-3D"],
              ]
  append_datasets: [] # List of datasets to append to the input/output projections for finetuning


